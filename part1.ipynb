{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект, Часть 1\n",
    "## Царькова Анастасия, Скоробогатов Денис, Анвардинов Шариф"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения этого кода потребуются nltk (токенизация и стоп-слова), requests (http) и readability (выделение основного контента)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import html\n",
    "import json\n",
    "import collections\n",
    "import multiprocessing\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "import readability\n",
    "\n",
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/pyos/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/pyos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. HackerNews\n",
    "\n",
    "**Задача**: выгрузить с HackerNews статьи со ссылками, комментарии, число лайков, даты публикации; перейти по каждой ссылке и сохранить основной контент статьи. Затем отфильтровать статьи, относящиеся к какому-то бренду."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HN_API_URL = 'https://hacker-news.firebaseio.com/v0/item/{}.json'\n",
    "HN_API_MAX = 'https://hacker-news.firebaseio.com/v0/maxitem.json'\n",
    "\n",
    "def traverse_comments(session, cur):\n",
    "    '''\n",
    "        Fetch & flatten the comment tree, starting with a given node.\n",
    "    \n",
    "        :param session: a requests session.\n",
    "        :param cur: the tree root.\n",
    "\n",
    "    '''\n",
    "    for id in cur.get('kids', []):\n",
    "        res = session.get(HN_API_URL.format(id)).json()\n",
    "        if res is None:\n",
    "            continue\n",
    "        if not res.get('deleted'):\n",
    "            yield res['text']\n",
    "        for text in traverse_comments(session, res):\n",
    "            yield text\n",
    "\n",
    "def get_text(session, url):\n",
    "    '''\n",
    "        Fetch & extract main content from an external source.\n",
    "        \n",
    "        :param session: a requests session.\n",
    "        :param url: the url of an article.\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        return readability.Document(session.get(url).text).summary()\n",
    "    except Exception:\n",
    "        return 'could not fetch {}'.format(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ускорения краулить будем в 10 потоков. Разумеется, было бы оптимальней использовать asyncio, но это бы слишком сильно замедлило процесс разработки. Поскольку API у HackerNews оказался очень уж медленным, а времени довольно мало, сохранять статьи будем за последнюю неделю, а не за полгода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles(i, w, n, output):\n",
    "    '''\n",
    "        Fetch HackerNews articles with comments.\n",
    "\n",
    "        :param i: the id of the worker thread.\n",
    "        :param w: total no. of worker threads. A thread will only check each `w`th id with phase `i`.\n",
    "        :param n: the last item observed to actually exist on HackerNews.\n",
    "        :param output: the queue to put the results into, or `None` when the thread exits.\n",
    "\n",
    "    '''\n",
    "    end = datetime.now() - timedelta(days=7)\n",
    "    session = requests.Session()\n",
    "    last_debug = n - i\n",
    "    for id in range(n - i, 0, -w):\n",
    "        try:\n",
    "            res = session.get(HN_API_URL.format(id)).json()\n",
    "            if res is None or res.get('type') != 'story' or res.get('deleted'):\n",
    "                # HackerNews has a single id namespace for articles and comments;\n",
    "                # in this loop, we're only interested in the former.\n",
    "                continue\n",
    "            time = datetime.fromtimestamp(res['time'])\n",
    "            if last_debug - id > 5000:\n",
    "                print('worker #{} - item #{} - {}'.format(i, id, time))\n",
    "                last_debug = id\n",
    "            if time < end:\n",
    "                break\n",
    "            output.put({\n",
    "                'id': res['id'],\n",
    "                'score': res['score'],\n",
    "                'url': res.get('url') or None,\n",
    "                'time': str(time),\n",
    "                'text': res.get('text', '') if not res.get('url') else get_text(session, res['url']),\n",
    "                'comments': list(traverse_comments(session, res)),\n",
    "            })\n",
    "        except Exception:\n",
    "            pass\n",
    "    print('worker #{} - done'.format(i))\n",
    "    output.put(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from 15444717\n",
      "worker #7 - item #15439520 - 2017-10-10 08:12:15\n",
      "worker #4 - item #15439513 - 2017-10-10 08:10:52\n",
      "worker #3 - item #15439424 - 2017-10-10 07:46:35\n",
      "worker #8 - item #15439339 - 2017-10-10 07:24:01\n",
      "worker #5 - item #15439492 - 2017-10-10 08:03:43\n",
      "worker #6 - item #15439381 - 2017-10-10 07:33:39\n",
      "worker #2 - item #15439125 - 2017-10-10 06:36:27\n",
      "worker #1 - item #15439556 - 2017-10-10 08:22:44\n",
      "worker #9 - item #15439308 - 2017-10-10 07:16:45\n",
      "worker #0 - item #15439367 - 2017-10-10 07:28:46\n",
      "worker #8 - item #15434109 - 2017-10-09 17:35:17\n",
      "worker #4 - item #15434263 - 2017-10-09 17:53:47\n",
      "worker #6 - item #15434021 - 2017-10-09 17:24:46\n",
      "worker #3 - item #15433584 - 2017-10-09 16:29:48\n",
      "worker #7 - item #15434100 - 2017-10-09 17:33:50\n",
      "worker #9 - item #15433988 - 2017-10-09 17:21:52\n",
      "worker #0 - item #15434187 - 2017-10-09 17:43:59\n",
      "worker #5 - item #15434262 - 2017-10-09 17:53:43\n",
      "worker #2 - item #15433665 - 2017-10-09 16:40:36\n",
      "worker #1 - item #15434296 - 2017-10-09 17:57:52\n",
      "worker #8 - item #15428739 - 2017-10-08 19:13:34\n",
      "worker #6 - item #15428621 - 2017-10-08 18:46:36\n",
      "worker #4 - item #15428933 - 2017-10-08 19:55:16\n",
      "worker #9 - item #15428728 - 2017-10-08 19:11:02\n",
      "worker #5 - item #15428762 - 2017-10-08 19:19:59\n",
      "worker #3 - item #15428014 - 2017-10-08 15:49:55\n",
      "worker #2 - item #15428365 - 2017-10-08 17:38:46\n",
      "worker #6 - item #15423451 - 2017-10-07 15:14:20\n",
      "worker #0 - item #15428987 - 2017-10-08 20:08:34\n",
      "worker #8 - item #15423499 - 2017-10-07 15:30:03\n",
      "worker #4 - item #15423473 - 2017-10-07 15:22:01\n",
      "worker #7 - item #15428860 - 2017-10-08 19:40:29\n",
      "worker #1 - item #15428986 - 2017-10-08 20:08:22\n",
      "worker #3 - item #15422634 - 2017-10-07 09:54:04\n",
      "worker #9 - item #15423478 - 2017-10-07 15:23:25\n",
      "worker #5 - item #15423402 - 2017-10-07 14:48:04\n",
      "worker #2 - item #15422805 - 2017-10-07 10:52:51\n",
      "worker #6 - item #15417891 - 2017-10-06 19:06:49\n",
      "worker #0 - item #15423617 - 2017-10-07 16:05:35\n",
      "worker #7 - item #15423610 - 2017-10-07 16:04:00\n",
      "worker #8 - item #15418109 - 2017-10-06 19:34:53\n",
      "worker #4 - item #15417953 - 2017-10-06 19:16:41\n",
      "worker #9 - item #15417888 - 2017-10-06 19:06:23\n",
      "worker #1 - item #15423566 - 2017-10-07 15:51:05\n",
      "worker #5 - item #15417922 - 2017-10-06 19:12:19\n",
      "worker #3 - item #15417414 - 2017-10-06 18:14:56\n",
      "worker #6 - item #15412711 - 2017-10-06 00:28:00\n",
      "worker #2 - item #15417595 - 2017-10-06 18:35:34\n",
      "worker #0 - item #15417937 - 2017-10-06 19:14:40\n",
      "worker #4 - item #15412713 - 2017-10-06 00:28:06\n",
      "worker #7 - item #15418170 - 2017-10-06 19:41:32\n",
      "worker #1 - item #15417836 - 2017-10-06 19:00:58\n",
      "worker #9 - item #15412658 - 2017-10-06 00:16:53\n",
      "worker #3 - item #15412164 - 2017-10-05 23:02:14\n",
      "worker #6 - item #15407531 - 2017-10-05 11:34:50\n",
      "worker #8 - item #15412769 - 2017-10-06 00:38:25\n",
      "worker #5 - item #15412842 - 2017-10-06 00:51:05\n",
      "worker #2 - item #15412085 - 2017-10-05 22:52:32\n",
      "worker #7 - item #15412910 - 2017-10-06 01:04:01\n",
      "worker #0 - item #15412587 - 2017-10-06 00:06:15\n",
      "worker #9 - item #15407218 - 2017-10-05 10:23:04\n",
      "worker #1 - item #15412646 - 2017-10-06 00:14:43\n",
      "worker #4 - item #15407453 - 2017-10-05 11:21:58\n",
      "worker #6 - item #15402261 - 2017-10-04 20:32:56\n",
      "worker #8 - item #15407429 - 2017-10-05 11:17:42\n",
      "worker #3 - item #15406804 - 2017-10-05 08:18:32\n",
      "worker #5 - item #15407532 - 2017-10-05 11:34:52\n",
      "worker #2 - item #15406625 - 2017-10-05 07:06:14\n",
      "worker #0 - item #15407177 - 2017-10-05 10:13:59\n",
      "worker #8 - item #15401999 - 2017-10-04 20:06:12\n",
      "worker #9 - item #15401668 - 2017-10-04 19:28:52\n",
      "worker #7 - item #15407620 - 2017-10-05 11:54:22\n",
      "worker #6 - item #15396881 - 2017-10-04 01:51:39\n",
      "worker #5 - item #15402132 - 2017-10-04 20:21:42\n",
      "worker #2 - item #15401445 - 2017-10-04 18:59:30\n",
      "worker #3 - item #15401324 - 2017-10-04 18:46:45\n",
      "worker #1 - item #15407326 - 2017-10-05 10:53:17\n",
      "worker #6 - done\n",
      "worker #0 - item #15401757 - 2017-10-04 19:40:04\n",
      "worker #8 - item #15396539 - 2017-10-04 00:52:56\n",
      "worker #4 - item #15401673 - 2017-10-04 19:29:21\n",
      "worker #8 - done\n",
      "worker #9 - item #15396228 - 2017-10-04 00:09:46\n",
      "worker #3 - item #15396004 - 2017-10-03 23:43:10\n",
      "worker #3 - done\n",
      "worker #1 - item #15401906 - 2017-10-04 19:57:44\n",
      "worker #2 - item #15396145 - 2017-10-03 23:59:52\n",
      "worker #7 - item #15402280 - 2017-10-04 20:34:14\n",
      "worker #9 - done\n",
      "worker #2 - done\n",
      "worker #4 - item #15396403 - 2017-10-04 00:34:09\n",
      "worker #5 - item #15396742 - 2017-10-04 01:29:11\n",
      "worker #4 - done\n",
      "worker #5 - done\n",
      "worker #1 - item #15396436 - 2017-10-04 00:38:50\n",
      "worker #0 - item #15396437 - 2017-10-04 00:38:53\n",
      "worker #7 - item #15397000 - 2017-10-04 02:11:39\n",
      "worker #7 - done\n",
      "worker #1 - done\n",
      "worker #0 - done\n"
     ]
    }
   ],
   "source": [
    "w = 10\n",
    "n = requests.get(HN_API_MAX).json()\n",
    "print('Starting from {}'.format(n))\n",
    "\n",
    "data = dict()\n",
    "output = multiprocessing.Queue()\n",
    "processes = [multiprocessing.Process(target=get_articles, args=(i, w, n, output)) for i in range(w)]\n",
    "for p in processes:\n",
    "    p.start()\n",
    "\n",
    "try:\n",
    "    while w:\n",
    "        res = output.get()\n",
    "        if res is None:\n",
    "            w -= 1\n",
    "        else:\n",
    "            data[res.pop('id')] = res\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "finally:\n",
    "    with open('hn-corpus.txt', 'w') as file:\n",
    "        json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6890"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('hn-corpus.txt', 'r') as file:\n",
    "    data = json.load(file)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-10-03 22:39:13'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(v['time'] for v in data.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем что-то делать, из статей и комментариев следует выкинуть HTML-разметку и капитализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags = re.compile(r'(<[^>]*>|\\s)+')\n",
    "for k, v in data.items():\n",
    "    v['text'] = html.unescape(tags.sub(' ', v['text'].lower()))\n",
    "    v['comments'] = list(map(lambda x: html.unescape(tags.sub(' ', x.lower())), v['comments']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('hn-corpus-stripped.txt', 'w') as file:\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бренд выберем, например, Google. Здесь есть проблема: как понять, что статья относится именно к нему, а не, скажем, мельком упоминает его в контексте новости о каком-нибудь конкуренте? Задача выглядит довольно сложной, так что решать ее не будем, а просто выберем статьи с хоть одним упоминанием."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brand_data = {k: v for k, v in data.items() if re.search(r'\\bgoogle\\b', v['text'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Исследование выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сколько постов в собранном корпусе?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1311"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brand_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Из каких источников состоит корпус?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    medium.com 83\n",
      "                techcrunch.com 35\n",
      "                   nytimes.com 26\n",
      "                hackernoon.com 23\n",
      "               arstechnica.com 19\n",
      "                     wired.com 18\n",
      "               theguardian.com 17\n",
      "            securityaffairs.co 16\n",
      "               theatlantic.com 15\n",
      "                  betanews.com 14\n",
      "                 bloomberg.com 12\n",
      "             theregister.co.uk 12\n",
      "                       eff.org 12\n",
      "                        goo.gl 10\n",
      "                     zdnet.com 9\n",
      "          technologyreview.com 9\n",
      "               venturebeat.com 9\n",
      "             bitsandscrews.com 8\n",
      "                        qz.com 8\n",
      "              en.wikipedia.org 8\n",
      "               fastcompany.com 7\n",
      "                      cnbc.com 7\n",
      "           businessinsider.com 7\n",
      "                   blog.google 7\n",
      "                   twitter.com 7\n",
      "                      cnet.com 7\n",
      "             blog.alexellis.io 7\n",
      "                  politico.com 6\n",
      "        medium.facilelogin.com 6\n",
      "                    nature.com 6\n"
     ]
    }
   ],
   "source": [
    "def hostname(url):\n",
    "    host = urlparse(url).hostname\n",
    "    if host.startswith('www.'):\n",
    "        return host[4:]\n",
    "    return host\n",
    "\n",
    "hosts = collections.Counter(hostname(v['url']) for k, v in brand_data.items() if v['url'])\n",
    "for host, count in hosts.most_common(30):\n",
    "    print('{: >30} {}'.format(host, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Какие слова (не считая стоп-слов) встречаются чаще всего?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Небольшое отступление: у nltk очень странное поведение по отношению к стоп-словам. Когда строили их список, апостроф явно считали разделителем. Например, там есть такие \"огрызки\", как `wasn` (очевидно, от `wasn't`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ma',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'wouldn']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk.word_tokenize` же осведомлен о правиле `<P> not -> <P>n't` и выделяет `n't` отдельным токеном:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['must', \"n't\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"mustn't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поэтому при токенизации апостроф следует заменить на пробел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                like 0.004919924727830389\n",
      "                 one 0.004555320505713497\n",
      "              google 0.004259172586524675\n",
      "              people 0.004081335016961537\n",
      "                 new 0.003988323735809269\n",
      "                data 0.0036371131381783034\n",
      "                also 0.003428023778148004\n",
      "               would 0.0033037607065285733\n",
      "                time 0.0032844143600489015\n",
      "             element 0.0032591152915754846\n",
      "                 use 0.0032137257863731777\n",
      "                 get 0.0030396086680561313\n",
      "                make 0.0024488010101769223\n",
      "                even 0.0024130846782144514\n",
      "               could 0.0023185852165637467\n",
      "                work 0.0022560816356294224\n",
      "                need 0.0021615821739787177\n",
      "               using 0.002159349903231063\n",
      "                 way 0.0021310744737607734\n",
      "               first 0.002082708607561594\n"
     ]
    }
   ],
   "source": [
    "def get_word_counts(brand_data, stopwords):\n",
    "    texts = []\n",
    "    for v in brand_data.values():\n",
    "        texts.append(v['text'])\n",
    "        texts.extend(v['comments'])\n",
    "    return collections.Counter(w for t in texts for w in nltk.word_tokenize(t.replace(\"'\", \" \")) if w.isalpha() and w not in stopwords)\n",
    "\n",
    "c = get_word_counts(brand_data, stopwords)\n",
    "for w, n in c.most_common(20):\n",
    "    print('{: >20} {}'.format(w, n / sum(c.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова довольно-таки негуглоспецифичные."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Из всех собранных ссылок, сколько ведут на блоги и новостные издания, а сколько – на github и другие неновостные издания?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу же возникает вопрос &mdash; как отличать \"новостные издания\" от \"неновостных изданий\"? Для некоторых доменов, таких как `medium.com`, корректный класс известен, но в общем случае сайты отличить довольно сложно. Некоторые люди к тому же, например, ведут блоги на GitHub, используя GitHub Pages или даже просто Markdown файлы &mdash; кроме как вручную это не классифицировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({None: 927, 'blog': 207, 'news': 157})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNOWN_NEWS = [\n",
    "    'businessinsider.com', 'forbes.com', 'wired.com', 'bloomberg.com',\n",
    "    'techcrunch.com', 'theverge.com', 'arstechnica.com', 'theguardian.com',\n",
    "    'engadget.com', 'washingtonpost.com', 'venturebeat.com', 'reuters.com',\n",
    "]\n",
    "\n",
    "KNOWN_BLOGS = [\n",
    "    'medium.com', 'hackernoon.com', '.blogspot.com',\n",
    "]\n",
    "\n",
    "def host_category(host):\n",
    "    if 'news.' in host:\n",
    "        return 'news'\n",
    "    if 'blog.' in host:\n",
    "        return 'blog'\n",
    "    for k in KNOWN_NEWS:\n",
    "        if host == k or (k.startswith('.') and host.endswith(k)):\n",
    "            return 'news'\n",
    "    for k in KNOWN_BLOGS:\n",
    "        if host == k or (k.startswith('.') and host.endswith(k)):\n",
    "            return 'blog'\n",
    "\n",
    "collections.Counter(host_category(h) for h in hosts.elements())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
