{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание с семинара 5\n",
    "\n",
    "## Анвардинов Шариф, Скоробогатов Денис, Царькова Анастасия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Данные и нормализация\n",
    "\n",
    "Возьмем случайные 100 текстов из коллекции статей NIPS (Neural Information Processing Systems) &mdash; https://www.kaggle.com/benhamner/nips-papers/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import random\n",
    "import pandas\n",
    "\n",
    "import nltk\n",
    "import nltk.stem\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "data = random.sample(pandas.read_csv('papers.csv')['paper_text'].tolist(), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статьи на английском, поэтому для лемматизации воспользуемся wordnet из nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pyos/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pyos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk.word_tokenize` разбивает сокращения наподобие \"mustn't\" на токены \"must\" и \"n't\", но при этом список стоп-слов для английского языка содержит куски вроде \"mustn\" и \"t\". Апострофы поэтому заменяем на пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.36 s, sys: 72 ms, total: 9.43 s\n",
      "Wall time: 9.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lm = nltk.stem.WordNetLemmatizer()\n",
    "sw = nltk.corpus.stopwords.words('english')\n",
    "normed = [' '.join(lm.lemmatize(w) for w in nltk.word_tokenize(t.lower().replace(\"'\", \" \")) if w not in sw) for t in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416053"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(x.split()) for x in normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TextRank\n",
    "\n",
    "`pip install gensim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim.summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 43s, sys: 11min 26s, total: 14min 10s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "textrank = {}\n",
    "for text in normed:\n",
    "    for i, word in enumerate(gensim.summarization.keywords(text).split('\\n'), 1):\n",
    "        textrank[word] = textrank.get(word, 0) + 1. / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.683715632278051\tmodeling\n",
      "6.671925957352868\talgorithm\n",
      "4.734110093687647\tlearn\n",
      "4.557393777439278\tmodeled\n",
      "3.879231016731017\talgorithmic\n",
      "3.4032746161537397\tlearned\n",
      "3.3278941071923533\tmodel\n",
      "3.204993801676846\tnetwork\n",
      "3.0604089320144423\tsetting\n",
      "2.9828754578754575\tlearns\n",
      "2.840051438263003\ttrained\n",
      "2.83718517860182\tlearning\n",
      "2.7910931174089066\tmodelling\n",
      "2.765129277408277\tset\n",
      "2.541970606569544\testimated\n",
      "2.3980212008638335\tstate\n",
      "2.338760396012371\tcompute\n",
      "2.326859272518663\tcomputing\n",
      "2.263645981844416\ttraining\n",
      "2.2372757695126118\tfunctional\n"
     ]
    }
   ],
   "source": [
    "for word, weight in heapq.nlargest(20, textrank.items(), key=lambda p: p[1]):\n",
    "    print(weight, word, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordle требует Java-плагин для браузера. Чтобы избежать его установки, воспользуемся аналогичным сервисом https://www.wordclouds.com. Для построения облака нам потребуется tsv-файл со строками (целочисленный вес, слово или фраза). Коэффициенты подобраны вручную для оптимальных эстетических качеств визуализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('textrank.txt', 'w') as out:\n",
    "    for word, weight in heapq.nlargest(100, textrank.items(), key=lambda p: p[1]):\n",
    "        print(int(3 * weight * 1.2 ** weight), word.replace(' ', '~'), sep='\\t', file=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](textrank.png)\n",
    "\n",
    "Смысл такой визуализации, честно говоря, не очевиден. Тем не менее, выделенные ключевые слова вполне подходят под тематику конференции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. RAKE\n",
    "\n",
    "`pip install python-rake`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.83 s, sys: 0 ns, total: 4.83 s\n",
      "Wall time: 4.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rake = {}\n",
    "rake_object = RAKE.Rake(sw)\n",
    "for text in normed:\n",
    "    for i, (word, weight) in enumerate(rake_object.run(text), 1):\n",
    "        if weight > 0:\n",
    "            rake[word] = rake.get(word, 0) + weight / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6651135295703305\te\n",
      "3.119121367521368\tac\n",
      "2.851336453959378\thowever\n",
      "2.71758855275812\tpaper\n",
      "2.2300233948669894\tg\n",
      "1.9193139733266753\tedu\n",
      "1.8209148349801705\tmit\n",
      "1.5833333333333333\tarizona\n",
      "1.5558873710591428\tinc\n",
      "1.5522520391989163\tparticular\n",
      "1.3782773628244749\texample\n",
      "1.3465651650799269\tb\n",
      "1.341868217173073\tx\n",
      "1.3328144078144077\tstanford\n",
      "1.2577109495483587\tcalifornia institute technology\n",
      "1.125\tucl\n",
      "1.1225401763331107\tp\n",
      "1.03607648919574\tst\n",
      "1.0114942528735633\thughes\n",
      "1.0058780260830875\tj\n"
     ]
    }
   ],
   "source": [
    "for word, weight in heapq.nlargest(20, rake.items(), key=lambda p: p[1]):\n",
    "    print(weight, word, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ничего хорошего не вышло &mdash; выделились в основном одиночные символы (видимо, обозначения из формул), домены первого уровня в адресах авторов, да несколько популярных слов (\"example\", \"particular\", \"paper\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Меры ассоциации биграм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем применять все меры сразу в линейной комбинации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 908 ms, sys: 0 ns, total: 908 ms\n",
      "Wall time: 907 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bigrams = {}\n",
    "measures = nltk.collocations.BigramAssocMeasures()\n",
    "for text in normed:\n",
    "    finder = nltk.collocations.BigramCollocationFinder.from_words(w for w in text.split() if w.isalnum())\n",
    "    finder.apply_freq_filter(5)\n",
    "    for measure, w in {measures.raw_freq: 1, measures.student_t: 1, measures.pmi: 1, measures.likelihood_ratio: 1, measures.chi_sq: 1}.items():\n",
    "        for bigram, score in finder.score_ngrams(measure):\n",
    "            bigrams[' '.join(bigram)] = bigrams.get(' '.join(bigram), 0) + score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48160.182377200596\tet al\n",
      "26972.252254864212\tmachine learning\n",
      "17419.444892701656\tneural network\n",
      "13537.78399558378\tarxiv preprint\n",
      "13305.721693496747\tinformation processing\n",
      "12639.277159118326\tneural information\n",
      "8585.11721438285\tadvance neural\n",
      "8416.304119794222\tfixed point\n",
      "8188.8857060570535\tprocessing system\n",
      "8067.827774796364\tsample complexity\n",
      "7250.012599117737\tmonte carlo\n",
      "7005.426958695783\tstandard deviation\n",
      "6848.115285344084\tupper bound\n",
      "6747.228226716034\tdynamic programming\n",
      "6672.4962010405825\tsupport vector\n",
      "6513.642299393167\tindian buffet\n",
      "6437.789075463936\treinforcement learning\n",
      "6349.209189542107\tgradient descent\n",
      "6061.768242062847\trandom field\n",
      "5891.006442047586\tlogistic regression\n"
     ]
    }
   ],
   "source": [
    "for word, weight in heapq.nlargest(20, bigrams.items(), key=lambda p: p[1]):\n",
    "    print(weight, word, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigrams.txt', 'w') as out:\n",
    "    for word, weight in heapq.nlargest(100, bigrams.items(), key=lambda p: p[1]):\n",
    "        print(int(weight ** 0.5 - 50), word.replace(' ', '~'), sep='\\t', file=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](bigrams.png)\n",
    "\n",
    "Среди биграм лучше всего выделились самые популярные (например, \"et al\" &mdash; \"и др. [авторы]\"). Явно надо подправить коэффициенты при комбинировании метрик. В список так же попали некоторые термины из машинного обучения, так что в целом это можно считать как успех."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TF-IDF\n",
    "\n",
    "`pip install scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.9 s, sys: 572 ms, total: 20.5 s\n",
      "Wall time: 20.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf = {}\n",
    "tfidf_transform = sklearn.feature_extraction.text.TfidfVectorizer(ngram_range=(1,2))\n",
    "tfidf_data = tfidf_transform.fit_transform(normed).todense()\n",
    "tfidf_phrases = tfidf_transform.get_feature_names()\n",
    "for row in tfidf_data:\n",
    "    row = row.tolist()[0]\n",
    "    for phrase, score in sorted(zip(range(len(row)), row), key=lambda p: p[1]):\n",
    "        tfidf[tfidf_phrases[phrase]] = tfidf.get(tfidf_phrases[phrase], 0) + score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.720232516137132\talgorithm\n",
      "3.598820523903939\tmodel\n",
      "3.014689996652276\tnetwork\n",
      "2.7597894925855226\tlearning\n",
      "2.5227077981066586\tdata\n",
      "2.3625972901459416\tset\n",
      "2.3596470748711837\tfunction\n",
      "2.286311807170011\tmatrix\n",
      "2.2595592999802037\timage\n",
      "2.1263697929274428\tstate\n",
      "2.0785566158930284\ttime\n",
      "1.956045974226798\t10\n",
      "1.885068370904615\tdistribution\n",
      "1.8794562581906187\tinput\n",
      "1.844576854104746\ttraining\n",
      "1.8119962758633\tusing\n",
      "1.779566947039452\tproblem\n",
      "1.7400146896722173\txi\n",
      "1.7389578724681898\terror\n",
      "1.737494316957606\tfigure\n"
     ]
    }
   ],
   "source": [
    "for word, weight in heapq.nlargest(20, tfidf.items(), key=lambda p: p[1]):\n",
    "    print(weight, word, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tfidf.txt', 'w') as out:\n",
    "    for word, weight in heapq.nlargest(100, tfidf.items(), key=lambda p: p[1]):\n",
    "        print(int(3 * weight * 1.3 ** weight), word.replace('_', '~'), sep='\\t', file=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](tfidf.png)\n",
    "\n",
    "Ни одной биграммы в топ 100 не попало."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
